{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7291c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from utils import misc\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from datasets import build_dataset_from_cfg\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_o3dpcd(points, colour=None):\n",
    "    if type(points) == list:\n",
    "        pcds = []\n",
    "        for pointcloud in points:\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(pointcloud[:,:3])\n",
    "            pcds.append(pcd)\n",
    "        return pcds\n",
    "    else:\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points[:,:3])\n",
    "        if colour:\n",
    "            pcd.paint_uniform_color(np.array(colour))\n",
    "        return pcd\n",
    "    \n",
    "def boxpts_to_o3dbox(boxpts, color=[1,0,0]):\n",
    "    o3dpts = o3d.utility.Vector3dVector(boxpts)\n",
    "    o3dbox = o3d.geometry.OrientedBoundingBox().create_from_points(o3dpts)\n",
    "    o3dbox.color = np.array(color)\n",
    "    return o3dbox\n",
    "\n",
    "def opd_to_boxpts(box):\n",
    "    \"\"\"\n",
    "    Takes an array containing [x,y,z,l,w,h,r], and returns an [8, 3] matrix that \n",
    "    represents the [x, y, z] for each 8 corners of the box.\n",
    "    \n",
    "    Note: Openpcdet __getitem__ gt_boxes are in the format [x,y,z,l,w,h,r,alpha]\n",
    "    where alpha is \"observation angle of object, ranging [-pi..pi]\"\n",
    "    \"\"\"\n",
    "    # To return\n",
    "    corner_boxes = np.zeros((8, 3))\n",
    "\n",
    "    translation = box[0:3]\n",
    "    l, w, h = box[3], box[4], box[5] # waymo, nusc, kitti is all l,w,h after OpenPCDet processing\n",
    "    rotation = box[6]\n",
    "\n",
    "    # Create a bounding box outline\n",
    "    bounding_box = np.array([[l/2, w/2, h/2],\n",
    "                             [l/2, -w/2, h/2],\n",
    "                             [-l/2, w/2, h/2],\n",
    "                             [-l/2, -w/2, h/2],\n",
    "                             [l/2, w/2, -h/2],\n",
    "                             [l/2, -w/2, -h/2],\n",
    "                             [-l/2, w/2, -h/2],\n",
    "                             [-l/2, -w/2, -h/2]])\n",
    "\n",
    "    # Standard 3x3 rotation matrix around the Z axis\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(rotation), np.sin(rotation), 0.0],\n",
    "        [-np.sin(rotation), np.cos(rotation), 0.0],\n",
    "        [0.0, 0.0, 1.0]])\n",
    "    vcbox = bounding_box @ rotation_matrix\n",
    "    vcbox += box[:3]\n",
    "    \n",
    "    return vcbox\n",
    "\n",
    "def merge_new_config(config, new_config):\n",
    "    for key, val in new_config.items():\n",
    "        if not isinstance(val, dict):\n",
    "            if key == '_base_':\n",
    "                with open(new_config['_base_'], 'r') as f:\n",
    "                    try:\n",
    "                        val = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                    except:\n",
    "                        val = yaml.load(f)\n",
    "                config[key] = EasyDict()\n",
    "                merge_new_config(config[key], val)\n",
    "            else:\n",
    "                config[key] = val\n",
    "                continue\n",
    "        if key not in config:\n",
    "            config[key] = EasyDict()\n",
    "        merge_new_config(config[key], val)\n",
    "    return config\n",
    "\n",
    "def cfg_from_yaml_file(cfg_file):\n",
    "    config = EasyDict()\n",
    "    with open(cfg_file, 'r') as f:\n",
    "        try:\n",
    "            new_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        except:\n",
    "            new_config = yaml.load(f)\n",
    "    merge_new_config(config=config, new_config=new_config)        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470934c",
   "metadata": {},
   "source": [
    "# PROTOTYPE - FIXED INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ee1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/pointr/models')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import misc\n",
    "from extensions.chamfer_dist import ChamferDistanceL2\n",
    "from utils.transform import rot_from_heading, rotate_points_along_z\n",
    "from utils.losses import geodesic_distance\n",
    "from utils.bbox_utils import get_dims, get_bbox_from_keypoints\n",
    "from utils.sampling import get_partial_mesh_batch\n",
    "\n",
    "def normalize_vector( v, return_mag =False):\n",
    "    batch=v.shape[0]\n",
    "    v_mag = torch.sqrt(v.pow(2).sum(1))# batch\n",
    "    v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([1e-8]).cuda()))\n",
    "    v_mag = v_mag.view(batch,1).expand(batch,v.shape[1])\n",
    "    v = v/v_mag\n",
    "    if(return_mag==True):\n",
    "        return v, v_mag[:,0]\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "# u, v batch*n\n",
    "def cross_product( u, v):\n",
    "    batch = u.shape[0]\n",
    "    #print (u.shape)\n",
    "    #print (v.shape)\n",
    "    i = u[:,1]*v[:,2] - u[:,2]*v[:,1]\n",
    "    j = u[:,2]*v[:,0] - u[:,0]*v[:,2]\n",
    "    k = u[:,0]*v[:,1] - u[:,1]*v[:,0]\n",
    "        \n",
    "    out = torch.cat((i.view(batch,1), j.view(batch,1), k.view(batch,1)),1)#batch*3\n",
    "        \n",
    "    return out\n",
    "\n",
    "def compute_rotation_matrix_from_ortho6d(ortho6d):\n",
    "    x_raw = ortho6d[:,0:3]#batch*3\n",
    "    y_raw = ortho6d[:,3:6]#batch*3\n",
    "        \n",
    "    x = normalize_vector(x_raw) #batch*3\n",
    "    z = cross_product(x,y_raw) #batch*3\n",
    "    z = normalize_vector(z)#batch*3\n",
    "    y = cross_product(z,x)#batch*3\n",
    "        \n",
    "    x = x.view(-1,3,1)\n",
    "    y = y.view(-1,3,1)\n",
    "    z = z.view(-1,3,1)\n",
    "    matrix = torch.cat((x,y,z), 2) #batch*3*3\n",
    "    return matrix\n",
    "\n",
    "def conv_layers(layer_dims, last_as_conv=False):\n",
    "    \n",
    "    in_channels = layer_dims[0]\n",
    "    conv_layers = []\n",
    "    for out_channel in layer_dims[1:]:\n",
    "        if out_channel == layer_dims[-1] and last_as_conv:\n",
    "            conv_layers.append(nn.Conv1d(in_channels, out_channel, kernel_size=1))\n",
    "            break\n",
    "            \n",
    "        conv_layers += [nn.Conv1d(in_channels, out_channel, kernel_size=1),\n",
    "                        nn.BatchNorm1d(out_channel),\n",
    "                        nn.ReLU()]\n",
    "        in_channels = out_channel\n",
    "    return nn.Sequential(*conv_layers)\n",
    "\n",
    "def fc_layers(layer_dims, last_as_linear=True):\n",
    "    \n",
    "    in_channels = layer_dims[0]\n",
    "    layers = []\n",
    "    for out_channel in layer_dims[1:]:\n",
    "        if out_channel == layer_dims[-1] and last_as_linear:\n",
    "            layers.append(nn.Linear(in_channels,out_channel))\n",
    "            break\n",
    "            \n",
    "        layers += [nn.Linear(in_channels,out_channel),\n",
    "                    nn.ReLU(inplace=True)]\n",
    "        in_channels = out_channel            \n",
    "        \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        # 3, 64, 128, 256, 256, 512\n",
    "        self.mlp_conv1 = nn.Sequential(\n",
    "            nn.Conv1d(dims[0],dims[1],1),\n",
    "            nn.BatchNorm1d(dims[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(dims[1],dims[2],1)\n",
    "        )\n",
    "        self.mlp_conv2 = nn.Sequential(\n",
    "            nn.Conv1d(dims[3],dims[4],1),\n",
    "            nn.BatchNorm1d(dims[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(dims[4],dims[5],1)\n",
    "        )\n",
    "    def forward(self, x, n, keepdims=False):\n",
    "        # Pytorch is (B,C,N) format\n",
    "\n",
    "        feature = self.mlp_conv1(x)  # B 256 n\n",
    "        feature_global = torch.max(feature,dim=2,keepdim=True)[0]  # B 256 1\n",
    "        feature = torch.cat([feature_global.expand(-1,-1,n), feature], dim=1)# B 512 n\n",
    "        feature = self.mlp_conv2(feature) # B 1024 n\n",
    "        feature_global = torch.max(feature,dim=2,keepdim=keepdims)[0]\n",
    "        \n",
    "        return feature_global\n",
    "\n",
    "class PartialSC_VC(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.sel_k = 30 # select nearest 30 points to each input point\n",
    "        \n",
    "        self.number_coarse = 1024\n",
    "        self.pose_encoder = nn.Sequential(\n",
    "            nn.Conv1d(3,64,1),            \n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(64,128,1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv1d(128,1024,1),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        )\n",
    "        self.pose_fc = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 9)            \n",
    "        )\n",
    "\n",
    "        self.encoder = FeatureEncoder([3, 128, 256, 512, 512, self.number_coarse])\n",
    "        self.shape_fc = fc_layers([1024, 1024, 1024, 3*self.number_coarse], last_as_linear=True) # canonical shape\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv1d(1024+3+2,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,3,1)\n",
    "        )\n",
    "        \n",
    "        self.build_loss_func()\n",
    "\n",
    "    def build_loss_func(self):\n",
    "        self.loss_coarse = ChamferDistanceL2()\n",
    "        self.loss_partial = ChamferDistanceL2()\n",
    "        self.loss_translation = nn.SmoothL1Loss(reduction='none')\n",
    "        self.loss_dims = nn.SmoothL1Loss(reduction='none')    \n",
    "\n",
    "    def get_loss(self, ret_dict, in_dict):     \n",
    "        gt_boxes = in_dict['gt_boxes']\n",
    "\n",
    "        loss_dict = {}        \n",
    "        pred_box = get_bbox_from_keypoints(ret_dict['coarse'], gt_boxes) # B 7        \n",
    "        loss_dict['dims'] = self.loss_dims(gt_boxes[:,3:6].cuda(), pred_box[:,3:6]).mean()\n",
    "\n",
    "        # Translation loss\n",
    "        gt_centres = gt_boxes[:,:3]\n",
    "        loss_dict['translation'] = self.loss_translation(gt_centres.cuda(), ret_dict['reg_centre']).mean()\n",
    "        \n",
    "        # Rotation loss\n",
    "        gt_headings = gt_boxes[:,-1]\n",
    "        gt_rmats = rot_from_heading(gt_headings)\n",
    "        theta = geodesic_distance(ret_dict['reg_rot'], gt_rmats)\n",
    "        loss_dict['rotation'] = theta.mean()\n",
    "\n",
    "        # Coarse loss - downsample complete with fps\n",
    "        if in_dict['training']:\n",
    "            ds_complete = misc.fps(in_dict['complete'], ret_dict['coarse'].shape[1])\n",
    "            loss_dict['coarse'] = self.loss_coarse(ret_dict['coarse'], ds_complete)            \n",
    "\n",
    "            pred_surface = get_partial_mesh_batch( in_dict['input'], ret_dict['coarse'], k=self.sel_k)\n",
    "            gt_surface = get_partial_mesh_batch( in_dict['input'], ds_complete, k=self.sel_k)\n",
    "            loss_dict['partial'] = self.loss_partial(pred_surface, gt_surface)                        \n",
    "\n",
    "        return loss_dict\n",
    "\n",
    "    def forward(self, in_dict):\n",
    "        ret = {}\n",
    "\n",
    "        bs , n , _ = in_dict['input'].shape\n",
    "        pc = in_dict['input']\n",
    "    \n",
    "        # Frustum angle is anti-clockwise about x axis (since +y is to the left, following RHS convention)\n",
    "        frustum_angle = torch.atan2(in_dict['input'][:,:,1].mean(dim=1), in_dict['input'][:,:,0].mean(dim=1))\n",
    "        pc_fview = rotate_points_along_z(in_dict['input'], -frustum_angle)\n",
    "    \n",
    "        # Centre pointcloud on mean of the points\n",
    "        pts_mean = pc_fview.mean(dim=1).unsqueeze(1)\n",
    "        pts_meancentered = pc_fview - pts_mean\n",
    "    \n",
    "        # Regress the relative pose\n",
    "        pose_feat = self.pose_encoder(pts_meancentered.permute(0,2,1)).view(bs, -1)  # B 256 n\n",
    "        rel_pose = self.pose_fc(pose_feat)\n",
    "        trans = rel_pose[:,:3].unsqueeze(1) # B 1 3\n",
    "        centre = pts_mean + trans\n",
    "        rot6d = rel_pose[:,3:9]\n",
    "        rot_mat = compute_rotation_matrix_from_ortho6d(rot6d)     \n",
    "        \n",
    "        pc_cn = torch.matmul(pc_fview - centre, rot_mat.permute(0,2,1))     \n",
    "    \n",
    "        # encoder\n",
    "        feature_global = self.encoder(pc_cn.permute(0,2,1), n)  # B 1024\n",
    "        coarse = self.shape_fc(feature_global).reshape(-1,self.number_coarse,3) # B coarse_pts 3            \n",
    "        coarse_vc = torch.matmul(coarse, rot_mat) + centre\n",
    "        \n",
    "        # Bring points back to sensor view\n",
    "        ret['coarse'] = rotate_points_along_z(coarse_vc.contiguous(), frustum_angle)\n",
    "        \n",
    "        # Bring regressed frustum view rotation/centroid to sensor view rotation/centroid\n",
    "        ret['reg_rot'] = torch.matmul(rot_mat, rot_from_heading(frustum_angle))\n",
    "        ret['reg_centre'] = rotate_points_along_z(centre, frustum_angle).squeeze(1)\n",
    "                \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e460e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 00:30:42,093 - VC_DATASET - INFO - Complete collecting files for val. Total views: 1360\n",
      "  0%|                                                        | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                        | 0/85 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device=1\n",
    "torch.cuda.set_device(device) # set to gpu 1080 whenever .cuda() is used\n",
    "\n",
    "cfg_file = '/PoinTr/cfgs/VC_models/PCN.yaml'\n",
    "config = cfg_from_yaml_file(cfg_file)\n",
    "\n",
    "data_subset = config.dataset.val\n",
    "data_subset.others.num_inputs = 1\n",
    "# data_subset._base_.USE_NVIEWS_PER_MODEL = 1\n",
    "\n",
    "dataset = build_dataset_from_cfg(data_subset._base_, data_subset.others)\n",
    "print(dataset.num_inputs)\n",
    "\n",
    "if data_subset._base_.NAME in ['VC','KITTI']:\n",
    "    if data_subset.others.fixed_input:\n",
    "        collate_fn = dataset.collate_fixed_input\n",
    "    else:            \n",
    "        collate_fn = dataset.collate_variable_input\n",
    "else:\n",
    "    collate_fn=None\n",
    "\n",
    "sampler = None\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
    "                                        shuffle = False, \n",
    "                                        drop_last = False,\n",
    "                                        num_workers = 0,\n",
    "                                        worker_init_fn=misc.worker_init_fn,\n",
    "                                        collate_fn=collate_fn)\n",
    "for idx, (taxonomy_ids, model_ids, data) in tqdm(enumerate(dataloader), total=len(dataloader)):        \n",
    "    label = data[2]\n",
    "    in_dict = {}    \n",
    "    if isinstance(data[0], list):\n",
    "        print('using siamese setup')\n",
    "        num_inputs = len(data[0])\n",
    "        for i in range(num_inputs):\n",
    "            in_dict[f'input_{i}'] = data[0][i].cuda()\n",
    "            in_dict[f'complete_{i}'] = data[1][i].cuda()\n",
    "            in_dict[f'gt_boxes_{i}'] = label[i]['gt_boxes'].cuda()\n",
    "            in_dict[f'num_pts_{i}'] = label[i]['num_pts']\n",
    "    else:\n",
    "        in_dict['input'] = data[0].cuda()\n",
    "        in_dict['gt_boxes'] = label['gt_boxes'].cuda() \n",
    "        in_dict['num_pts'] = label['num_pts']\n",
    "        in_dict['complete'] = data[1].cuda()\n",
    "        \n",
    "    in_dict['training'] = True   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b086fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005746603012084961\n"
     ]
    }
   ],
   "source": [
    "# npts = list(np.random.randint(50,2048, 1))\n",
    "# sim_data = torch.rand(1,sum(npts),3).cuda()\n",
    "# print(f'input shape = {sim_data.shape}')\n",
    "    \n",
    "# For final model, do some tweaking\n",
    "# - test adding dropout to prevent overfitting\n",
    "    \n",
    "model = PartialSC_VC(config)\n",
    "\n",
    "ckpt_path = '/PoinTr/PartialSC_VC.pth'\n",
    "if ckpt_path:    \n",
    "    state_dict = torch.load(ckpt_path, map_location=f'cuda:{device}')\n",
    "    base_ckpt = {k.replace(\"module.\", \"\"): v for k, v in state_dict['base_model'].items()}\n",
    "    model.load_state_dict(base_ckpt)\n",
    "    model.to(device)    \n",
    "\n",
    "model.eval()\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "ret = model(in_dict)\n",
    "print(time.time() - t0)\n",
    "# ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9639d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dims': tensor(0.0318, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " 'translation': tensor(0.0070, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " 'rotation': tensor(0.4022, device='cuda:1', grad_fn=<MeanBackward0>),\n",
       " 'coarse': tensor(0.0279, device='cuda:1', grad_fn=<AddBackward0>),\n",
       " 'partial': tensor(0.0173, device='cuda:1', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_loss(ret, in_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e91874",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feab5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_mesh(partial_pc, complete_pc, k=30, surface_pts=1024):\n",
    "    \"\"\"\n",
    "    For each point in the partial pc, we find the k nearest points in \n",
    "    the complete pc to keep. We take a set of all the points so there\n",
    "    are no duplicates. \n",
    "\n",
    "    partial_pc (torch N 3): Input sparse point cloud \n",
    "    complete_pc (torch N 3): Dense complete surface/ coarse prediction\n",
    "    k (int): Number of nearest points to sample\n",
    "\n",
    "    Returns pc (N 3)\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(partial_pc.shape) == 2, f'partial_pc shape is {partial_pc.shape}, must have shape (1024,3)'\n",
    "    surface_idx = []\n",
    "    partial_pc = torch.unique(partial_pc, dim=0)\n",
    "    for i in range(len(partial_pc)):\n",
    "        dist = torch.norm(complete_pc - partial_pc[i], dim=1, p=None)\n",
    "        knn = dist.topk(k, largest=False)\n",
    "        surface_idx.extend(knn[1].cpu().numpy())\n",
    "        \n",
    "    surface_idx = torch.tensor(list(set(surface_idx))).to(complete_pc.get_device())\n",
    "    sel_complete = torch.index_select(complete_pc, dim=0, index=surface_idx)\n",
    "    repeat_factor = int(np.ceil(surface_pts/sel_complete.shape[0]))\n",
    "    sel_sampled = sel_complete.repeat(repeat_factor, 1)[:surface_pts,:]\n",
    "    return sel_sampled\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "def partial_with_KDTree(partial_pc, complete_pc, k, surface_pts=1024):\n",
    "    \"\"\"\n",
    "    Same function but faster than get_partial_mesh, since it uses KDTree\n",
    "\n",
    "    partial_pc (numpy N 3): Input sparse point cloud \n",
    "    complete_pc (numpy N 3): Dense complete surface/ coarse prediction\n",
    "    k (int): Number of nearest points to sample\n",
    "\n",
    "    Returns pc (N 3)\n",
    "\n",
    "    \"\"\"\n",
    "    assert len(partial_pc.shape) == 2, f'partial_pc shape is {partial_pc.shape}, must have shape (1024,3)'\n",
    "    if partial_pc.requires_grad:\n",
    "        partial_pc = partial_pc.detach()\n",
    "    if isinstance(partial_pc, torch.Tensor):\n",
    "        partial_pc = partial_pc.cpu().numpy()\n",
    "    if complete_pc.requires_grad:\n",
    "        complete_pc = complete_pc.detach()\n",
    "    if isinstance(complete_pc, torch.Tensor):\n",
    "        complete_pc = complete_pc.cpu().numpy()\n",
    "\n",
    "    surface_idx = []\n",
    "    kd = cKDTree(complete_pc)\n",
    "    partial_pc = np.unique(partial_pc, axis=0)\n",
    "    for i in range(len(partial_pc)):\n",
    "        knn_idx = kd.query(partial_pc[i], k=k)[1]\n",
    "        surface_idx.extend(knn_idx)\n",
    "    \n",
    "    surface_idx = list(set(surface_idx))\n",
    "    sel_complete = complete_pc[surface_idx]\n",
    "    repeat_factor = int(np.ceil(surface_pts/sel_complete.shape[0]))    \n",
    "    sel_sampled = np.tile(sel_complete, [surface_pts, 1])[:surface_pts,:]\n",
    "    \n",
    "    return sel_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232a8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_idx = 0\n",
    "pc_np = ret['coarse'][sel_idx].detach().cpu().numpy()\n",
    "sel_sampled = partial_with_KDTree(in_dict['input'][sel_idx].cpu().numpy(), pc_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d08a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "out_coarse = ret['coarse'].detach().cpu().numpy()\n",
    "sel_idx = 0\n",
    "\n",
    "pc = out_coarse[sel_idx]\n",
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(sel_sampled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a35b360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dict['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be8c084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.29 s ± 45.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "sel_idx = 0\n",
    "def test1():\n",
    "    pc_torch = ret['coarse'][sel_idx]\n",
    "    sampled = get_partial_mesh(in_dict['input'][sel_idx], pc_torch, k=20, surface_pts=1024)\n",
    "    \n",
    "%timeit for x in range(100): test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4fb129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1114661693572998\n"
     ]
    }
   ],
   "source": [
    "sel_idx = 5\n",
    "import time\n",
    "t0 = time.time()\n",
    "pc_torch = ret['coarse'][sel_idx]\n",
    "sampled_t = get_partial_mesh(in_dict['input'][sel_idx], pc_torch, k=20, surface_pts=1024)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1039857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03335428237915039\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "pc_np = ret['coarse'][sel_idx].detach().cpu().numpy()\n",
    "sampled_n = partial_with_KDTree(in_dict['input'][sel_idx].cpu().numpy(), pc_np, k=20, surface_pts=1024)\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24227426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_n = partial_with_KDTree(in_dict['input'][sel_idx], ret['coarse'][sel_idx], k=20, surface_pts=1024)\n",
    "sampled_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7178977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_mesh_batch(batch_partial, batch_complete, k=20, surface_pts=1024):\n",
    "    \"\"\"\n",
    "    Runs \"partial_with_KDTree\" function but in batches.\n",
    "\n",
    "    partial_pc (B N 3): Input sparse point cloud\n",
    "    complete_pc (B N 3): Dense complete surface/ coarse prediction\n",
    "    k (int): Number of nearest points to sample\n",
    "\n",
    "    Returns pc (B N 3)\n",
    "    \"\"\"\n",
    "    print(\"using kdtree\")\n",
    "    surfaces = [partial_with_KDTree(partial, complete, k=k, surface_pts=surface_pts)[np.newaxis,...] for partial, complete in zip(batch_partial, batch_complete)]\n",
    "    return np.concatenate(surfaces, axis=0)\n",
    "\n",
    "def get_largest_cluster(pc, eps=0.4, min_points=1, istensor=False, total_pts=1024):\n",
    "    \"\"\"\n",
    "    partial_pc (N 3): Input sparse point cloud \n",
    "    Returns pc (N 3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sel_pc = convert_to_o3dpcd(pc)\n",
    "    labels = np.array(sel_pc.cluster_dbscan(eps=eps, min_points=min_points))\n",
    "    y = np.bincount(labels[labels >= 0])\n",
    "    value = np.argmax(y)\n",
    "    most_points = np.argwhere(labels == value)\n",
    "    f_pcd = sel_pc.select_by_index(most_points)\n",
    "    ret_pc = np.asarray(f_pcd.points)\n",
    "\n",
    "    ret_pc_sampled = np.tile(ret_pc, (int(np.ceil(total_pts/ret_pc.shape[0])), 1))[:total_pts,:]\n",
    "\n",
    "    return ret_pc_sampled\n",
    "\n",
    "def get_largest_cluster_batch(pc, eps=0.4, min_points=1, total_pts=1024):\n",
    "    \"\"\"\n",
    "    partial_pc (B N 3): Input sparse point cloud (Tensor)\n",
    "    Returns pc (B N 3)\n",
    "\n",
    "    \"\"\"   \n",
    "    clusters = [get_largest_cluster(p, eps=eps, min_points=min_points, total_pts=total_pts)[np.newaxis,...] for p in pc]\n",
    "    return np.concatenate(clusters, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcb8663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using kdtree\n"
     ]
    }
   ],
   "source": [
    "surfaces = get_partial_mesh_batch(in_dict['input'], ret['coarse'])\n",
    "cluster = get_largest_cluster_batch(surfaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d93e1fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_t = sampled_t.detach().cpu().numpy()\n",
    "np.allclose(sampled_n, sampled_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "690b8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(convert_to_o3dpcd(list(cluster)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ec2217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.17 s ± 54.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def test2():\n",
    "    pc_np = ret['coarse'][sel_idx].detach().cpu().numpy()\n",
    "    sampled = partial_with_KDTree(in_dict['input'][sel_idx].cpu().numpy(), pc_np, k=20, surface_pts=1024)\n",
    "    \n",
    "%timeit for x in range(100): test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92d3ef",
   "metadata": {},
   "source": [
    "# Attention modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ceef7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/ShiQiu0419/attentions_in_3D_detection/blob/main/backbone_module.py\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, C, r=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(C, C // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(C // r, C, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, C, _ = x.shape\n",
    "        out = self.squeeze(x).view(b, C)\n",
    "        out = self.excitation(out).view(b, C, 1)\n",
    "        return x * out.expand_as(x)\n",
    "    \n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    \"\"\" this function is used to achieve the channel attention module in CBAM paper\"\"\"\n",
    "    def __init__(self, C, ratio=8): # getting from the CBAM paper, ratio=16\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=C, out_channels=C // ratio, kernel_size=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels= C // ratio, out_channels=C, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        out1 = torch.mean(x, dim=-1, keepdim=True)  # b, c, 1\n",
    "        out1 = self.mlp(out1) # b, c, 1\n",
    "\n",
    "        out2 = nn.AdaptiveMaxPool1d(1)(x) # b, c, 1\n",
    "        out2 = self.mlp(out2) # b, c, 1\n",
    "\n",
    "        out = self.sigmoid(out1 + out2)\n",
    "\n",
    "        return out * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90340f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0006, 0.0013, 0.0006,  ..., 0.0015, 0.0011, 0.0012],\n",
       "        [0.0015, 0.0006, 0.0015,  ..., 0.0010, 0.0007, 0.0009],\n",
       "        [0.0009, 0.0013, 0.0012,  ..., 0.0006, 0.0008, 0.0006],\n",
       "        ...,\n",
       "        [0.0006, 0.0012, 0.0006,  ..., 0.0008, 0.0008, 0.0008],\n",
       "        [0.0014, 0.0006, 0.0014,  ..., 0.0015, 0.0012, 0.0012],\n",
       "        [0.0009, 0.0006, 0.0012,  ..., 0.0010, 0.0014, 0.0011]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = nn.Softmax(dim=1)\n",
    "sm(torch.rand(16,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5de5556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2792],\n",
       "         [0.4631],\n",
       "         [0.0650],\n",
       "         ...,\n",
       "         [0.0927],\n",
       "         [0.1690],\n",
       "         [0.0214]],\n",
       "\n",
       "        [[0.2022],\n",
       "         [0.3506],\n",
       "         [0.3996],\n",
       "         ...,\n",
       "         [0.3873],\n",
       "         [0.4483],\n",
       "         [0.1604]],\n",
       "\n",
       "        [[0.3080],\n",
       "         [0.3987],\n",
       "         [0.1682],\n",
       "         ...,\n",
       "         [0.2807],\n",
       "         [0.3572],\n",
       "         [0.2634]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0350],\n",
       "         [0.3573],\n",
       "         [0.0456],\n",
       "         ...,\n",
       "         [0.1554],\n",
       "         [0.3275],\n",
       "         [0.2671]],\n",
       "\n",
       "        [[0.2564],\n",
       "         [0.0556],\n",
       "         [0.1130],\n",
       "         ...,\n",
       "         [0.3959],\n",
       "         [0.3518],\n",
       "         [0.3027]],\n",
       "\n",
       "        [[0.2859],\n",
       "         [0.3261],\n",
       "         [0.3122],\n",
       "         ...,\n",
       "         [0.0898],\n",
       "         [0.3027],\n",
       "         [0.1494]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = ChannelAttentionModule(256)\n",
    "att(torch.rand(16,256,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332eb259",
   "metadata": {},
   "source": [
    "# Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfb382a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction_target(rot_gt, num_bins=2, period=2*np.pi, dir_offset=0.0):\n",
    "\n",
    "    \"\"\"\n",
    "    Taken from OpenPCDet, anchor_head_template.py        \n",
    "    Not sure what dir_offset is, but in OpenPCDet, they set it to pi/4\n",
    "\n",
    "    \"\"\"\n",
    "    val = rot_gt - dir_offset\n",
    "    offset_rot = val - torch.floor(val / period) * period\n",
    "\n",
    "    # Divide by period=pi. If angle less than pi, then it'll be less than 1\n",
    "    # If angle more than pi, then result is more than 1. So we floor it and \n",
    "    # it's a smart way of getting the classes\n",
    "    dir_cls_targets = torch.floor(offset_rot / (2 * np.pi / num_bins)).long()\n",
    "    dir_cls_targets = torch.clamp(dir_cls_targets, min=0, max=num_bins - 1)\n",
    "    dir_targets = torch.zeros(*list(dir_cls_targets.shape), num_bins).cuda()\n",
    "    dir_targets.scatter_(-1, dir_cls_targets.unsqueeze(dim=-1).long(), 1.0)\n",
    "    return dir_targets\n",
    "\n",
    "def test_dir_target(headings):\n",
    "    one_hot = get_direction_target(headings)\n",
    "    dir_lab = torch.argmax(one_hot, dim=1)\n",
    "    check = headings * dir_lab\n",
    "    print(check)\n",
    "    print(check < 0)\n",
    "    assert torch.all(check < 0), f\"headings=\\n{torch.stack([headings, dir_lab]).permute(1,0)}\"    \n",
    "    \n",
    "def test_rotm_to_heading(headings):\n",
    "    gt_rmat = rot_from_heading(headings)\n",
    "    mult = torch.matmul(gt_rmat, gt_rmat)    \n",
    "    gt_add = rot_from_heading(headings + headings)\n",
    "    matclose = torch.allclose(mult,gt_add, atol=1e-6)\n",
    "    if not matclose:\n",
    "        print(mult)\n",
    "        print(gt_add)\n",
    "    \n",
    "    z_from_rot = rotm_to_heading(mult)\n",
    "    z_added = headings + headings\n",
    "    z_added_grtpi = z_added > np.pi\n",
    "    z_added_letnegpi = z_added <= -np.pi\n",
    "    z_added[z_added_grtpi] -= 2*np.pi\n",
    "    z_added[z_added_letnegpi] += 2*np.pi\n",
    "\n",
    "    headclose = torch.allclose(z_from_rot, z_added, atol=1e-6)\n",
    "    if not headclose:\n",
    "        print('gt_boxes = ', gt_boxes[:,-1])\n",
    "        print('z_from_rot = ', z_from_rot)\n",
    "        print('z_added = ', z_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65985eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2954, 0.7741, 0.1584],\n",
      "         [0.7551, 0.3548, 0.6091],\n",
      "         [0.7667, 0.9050, 0.8529]],\n",
      "\n",
      "        [[0.4452, 0.5041, 0.3528],\n",
      "         [0.9073, 0.4671, 0.6447],\n",
      "         [0.7740, 0.1058, 0.5089]],\n",
      "\n",
      "        [[0.2188, 0.1755, 0.0774],\n",
      "         [0.5338, 0.5311, 0.8575],\n",
      "         [0.5730, 0.4951, 0.4175]],\n",
      "\n",
      "        [[0.6832, 0.2816, 0.7955],\n",
      "         [0.5549, 0.5363, 0.5143],\n",
      "         [0.3852, 0.7853, 0.1734]],\n",
      "\n",
      "        [[0.1175, 0.7590, 0.0921],\n",
      "         [0.3102, 0.3112, 0.7955],\n",
      "         [0.1418, 0.1719, 0.5091]],\n",
      "\n",
      "        [[0.2151, 0.9018, 0.8396],\n",
      "         [0.2959, 0.2092, 0.2264],\n",
      "         [0.9510, 0.7285, 0.6118]],\n",
      "\n",
      "        [[0.8687, 0.4731, 0.6261],\n",
      "         [0.4016, 0.5731, 0.4948],\n",
      "         [0.1436, 0.9237, 0.3084]],\n",
      "\n",
      "        [[0.1148, 0.2804, 0.6856],\n",
      "         [0.6291, 0.3324, 0.4254],\n",
      "         [0.6670, 0.3520, 0.8038]],\n",
      "\n",
      "        [[0.5232, 0.9408, 0.7949],\n",
      "         [0.6530, 0.5521, 0.0820],\n",
      "         [0.8782, 0.0681, 0.9177]],\n",
      "\n",
      "        [[0.1741, 0.5378, 0.5479],\n",
      "         [0.7656, 0.5148, 0.2273],\n",
      "         [0.4622, 0.4887, 0.3351]],\n",
      "\n",
      "        [[0.8691, 0.4440, 0.9655],\n",
      "         [0.1560, 0.8459, 0.5004],\n",
      "         [0.3739, 0.5543, 0.4157]],\n",
      "\n",
      "        [[0.9048, 0.1930, 0.9644],\n",
      "         [0.7968, 0.8336, 0.0188],\n",
      "         [0.7713, 0.0831, 0.6386]],\n",
      "\n",
      "        [[0.2535, 0.6465, 0.4423],\n",
      "         [0.7444, 0.3837, 0.0782],\n",
      "         [0.4653, 0.6448, 0.3196]],\n",
      "\n",
      "        [[0.2767, 0.2229, 0.8246],\n",
      "         [0.2935, 0.9926, 0.8350],\n",
      "         [0.8500, 0.9405, 0.9244]],\n",
      "\n",
      "        [[0.9294, 0.2225, 0.5484],\n",
      "         [0.1096, 0.8151, 0.6171],\n",
      "         [0.9818, 0.3640, 0.3156]],\n",
      "\n",
      "        [[0.4211, 0.5333, 0.2779],\n",
      "         [0.9235, 0.9261, 0.6123],\n",
      "         [0.4683, 0.4267, 0.7043]]], device='cuda:2')\n",
      "tensor([[[ 0.0315,  0.9995,  0.0000],\n",
      "         [-0.9995,  0.0315,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.9998,  0.0195,  0.0000],\n",
      "         [-0.0195,  0.9998,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.2188,  0.1755,  0.0774],\n",
      "         [ 0.5338,  0.5311,  0.8575],\n",
      "         [ 0.5730,  0.4951,  0.4175]],\n",
      "\n",
      "        [[ 0.6832,  0.2816,  0.7955],\n",
      "         [ 0.5549,  0.5363,  0.5143],\n",
      "         [ 0.3852,  0.7853,  0.1734]],\n",
      "\n",
      "        [[ 0.0243, -0.9997,  0.0000],\n",
      "         [ 0.9997,  0.0243,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[-1.0000,  0.0080,  0.0000],\n",
      "         [-0.0080, -1.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.8687,  0.4731,  0.6261],\n",
      "         [ 0.4016,  0.5731,  0.4948],\n",
      "         [ 0.1436,  0.9237,  0.3084]],\n",
      "\n",
      "        [[ 0.1148,  0.2804,  0.6856],\n",
      "         [ 0.6291,  0.3324,  0.4254],\n",
      "         [ 0.6670,  0.3520,  0.8038]],\n",
      "\n",
      "        [[ 0.5232,  0.9408,  0.7949],\n",
      "         [ 0.6530,  0.5521,  0.0820],\n",
      "         [ 0.8782,  0.0681,  0.9177]],\n",
      "\n",
      "        [[ 0.1741,  0.5378,  0.5479],\n",
      "         [ 0.7656,  0.5148,  0.2273],\n",
      "         [ 0.4622,  0.4887,  0.3351]],\n",
      "\n",
      "        [[ 0.0794,  0.9968,  0.0000],\n",
      "         [-0.9968,  0.0794,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.9048,  0.1930,  0.9644],\n",
      "         [ 0.7968,  0.8336,  0.0188],\n",
      "         [ 0.7713,  0.0831,  0.6386]],\n",
      "\n",
      "        [[-0.8236,  0.5672,  0.0000],\n",
      "         [-0.5672, -0.8236,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[-0.9965,  0.0831,  0.0000],\n",
      "         [-0.0831, -0.9965,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.9294,  0.2225,  0.5484],\n",
      "         [ 0.1096,  0.8151,  0.6171],\n",
      "         [ 0.9818,  0.3640,  0.3156]],\n",
      "\n",
      "        [[ 0.9238,  0.3828,  0.0000],\n",
      "         [-0.3828,  0.9238,  0.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000]]], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "rot_mat = torch.rand(16,3,3).cuda()\n",
    "pred_dirlab = torch.from_numpy(np.random.randint(0,2,16)).cuda()\n",
    "gt_dirlab = torch.argmax(get_direction_target(in_dict['gt_boxes'][:,-1]), dim=1)\n",
    "wrongdir_mask = pred_dirlab != gt_dirlab\n",
    "\n",
    "print(rot_mat)\n",
    "\n",
    "rot_mat[wrongdir_mask] = rot_from_heading(in_dict['gt_boxes'][:,-1])[wrongdir_mask]\n",
    "print(rot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66026bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 0],\n",
       "        [1, 0]], device='cuda:2')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([pred_dirlab, gt_dirlab]).permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94ed4b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/7675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    -0.002,      0.000,      0.000,      0.000,      0.000,     -3.060,\n",
      "            -0.005,     -3.131,      0.000,     -2.586,     -1.635,     -0.069,\n",
      "            -2.531,     -0.240,      0.000,     -0.037], device='cuda:2')\n",
      "tensor([ True, False, False, False, False,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True, False,  True], device='cuda:2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "headings=\ntensor([[    -0.002,      1.000],\n        [     0.019,      0.000],\n        [     1.614,      0.000],\n        [     1.491,      0.000],\n        [     0.393,      0.000],\n        [    -3.060,      1.000],\n        [    -0.005,      1.000],\n        [    -3.131,      1.000],\n        [     0.031,      0.000],\n        [    -2.586,      1.000],\n        [    -1.635,      1.000],\n        [    -0.069,      1.000],\n        [    -2.531,      1.000],\n        [    -0.240,      1.000],\n        [     0.049,      0.000],\n        [    -0.037,      1.000]], device='cuda:2')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4349/195043726.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gt_boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtest_dir_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4349/2329838133.py\u001b[0m in \u001b[0;36mtest_dir_target\u001b[0;34m(headings)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"headings=\\n{torch.stack([headings, dir_lab]).permute(1,0)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_rotm_to_heading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: headings=\ntensor([[    -0.002,      1.000],\n        [     0.019,      0.000],\n        [     1.614,      0.000],\n        [     1.491,      0.000],\n        [     0.393,      0.000],\n        [    -3.060,      1.000],\n        [    -0.005,      1.000],\n        [    -3.131,      1.000],\n        [     0.031,      0.000],\n        [    -2.586,      1.000],\n        [    -1.635,      1.000],\n        [    -0.069,      1.000],\n        [    -2.531,      1.000],\n        [    -0.240,      1.000],\n        [     0.049,      0.000],\n        [    -0.037,      1.000]], device='cuda:2')"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "for idx, (taxonomy_ids, model_ids, data) in tqdm(enumerate(dataloader), total=len(dataloader)):        \n",
    "    label = data[2]\n",
    "    gt_boxes = label['gt_boxes'].cuda() \n",
    "    \n",
    "    test_dir_target(gt_boxes[:,-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92ba9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 342/342 [01:41<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "combined_label = {}\n",
    "for mid in tqdm(dataset.model_ids, total=len(dataset.model_ids)):\n",
    "    combined_label[mid] = {}\n",
    "    for idx in range(40):        \n",
    "        label_file = dataset.label_path % (mid, idx)\n",
    "        with open(label_file, 'rb') as f:\n",
    "            combined_label[mid][idx] = pickle.load(f)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1151153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/PoinTr/data/VC/WAYMO_nviews-40/val/label.pkl', 'wb') as f:\n",
    "    pickle.dump(combined_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7941e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.transform import rotate_points_along_z\n",
    "\n",
    "# Frustum angle is anti-clockwise about x axis (since +y is to the left, following RHS convention)\n",
    "frustum_angle = torch.atan2(in_dict['input'][:,:,1].mean(dim=1), in_dict['input'][:,:,0].mean(dim=1))\n",
    "rot_pcs = rotate_points_along_z(in_dict['input'], -frustum_angle)\n",
    "gt_boxes = in_dict['gt_boxes'].clone().detach()\n",
    "gt_boxes[:,:3] = rotate_points_along_z(gt_boxes[:,:3].unsqueeze(1), -frustum_angle).squeeze(1)\n",
    "\n",
    "# For gt boxes, we also need to rotate the car's orientation for the box\n",
    "gt_boxes[:,-1] = gt_boxes[:,-1] - frustum_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "024b84a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize to double check\n",
    "pc_in = [convert_to_o3dpcd(rpc.cpu().numpy()) for rpc in in_dict['input']]\n",
    "# pc_in = [convert_to_o3dpcd(pc.cpu().numpy()) for pc in in_dict['input']]\n",
    "o3dbox = [boxpts_to_o3dbox(opd_to_boxpts(gbox.cpu().numpy())) for gbox in in_dict['gt_boxes']]\n",
    "\n",
    "o3d.visualization.draw_geometries(pc_in + o3dbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd411b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize to double check\n",
    "pc_in = [convert_to_o3dpcd(rpc.cpu().numpy()) for rpc in rot_pcs]\n",
    "# pc_in = [convert_to_o3dpcd(pc.cpu().numpy()) for pc in in_dict['input']]\n",
    "o3dbox = [boxpts_to_o3dbox(opd_to_boxpts(gbox.cpu().numpy())) for gbox in gt_boxes]\n",
    "\n",
    "o3d.visualization.draw_geometries(pc_in + o3dbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13d09e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d627e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize to double check\n",
    "siamese_idx = 1\n",
    "pc_num = 0\n",
    "\n",
    "pc_in = convert_to_o3dpcd(in_dict[f'input_{siamese_idx}'][pc_num,:,:].cpu().numpy())\n",
    "pc_com = convert_to_o3dpcd(in_dict[f'complete_{siamese_idx}'][pc_num,:,:].cpu().numpy(), [0.9,0.9,0.9])\n",
    "boxpts = opd_to_boxpts(in_dict[f'gt_boxes_{siamese_idx}'][pc_num,:].cpu().numpy())\n",
    "o3dgtbox = boxpts_to_o3dbox(boxpts, [1,0,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([pc_in, pc_com, o3dgtbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87fc28d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following are equivalent\n",
    "gt_boxes = in_dict['gt_boxes'].clone().detach()\n",
    "frustum_angle = torch.atan2(in_dict['input'][:,:,1].mean(dim=1), in_dict['input'][:,:,0].mean(dim=1))\n",
    "\n",
    "a = (rot_from_heading(gt_boxes[:,-1]) @ rot_from_heading(-frustum_angle))\n",
    "b = rot_from_heading(gt_boxes[:,-1] - frustum_angle)\n",
    "torch.allclose(a,b) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a337fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1950cdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_box = a @ rot_from_heading(frustum_angle)\n",
    "torch.allclose(orig_box, rot_from_heading(gt_boxes[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ca8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frustum_angle = torch.atan2(in_dict['input'][:,:,1].mean(dim=1), in_dict['input'][:,:,0].mean(dim=1))\n",
    "pc_fview = rotate_points_along_z(in_dict['input'], -frustum_angle)\n",
    "\n",
    "gt_boxes = in_dict['gt_boxes']\n",
    "gt_boxes[:,:3] = rotate_points_along_z(gt_boxes[:,:3].unsqueeze(1), -frustum_angle).squeeze(1)\n",
    "gt_boxes[:,-1] = gt_boxes[:,-1] - frustum_angle\n",
    "\n",
    "o3d_boxes = [boxpts_to_o3dbox(opd_to_boxpts(gt_boxes[i,:].cpu().numpy())) for i in range(gt_boxes.shape[0])]\n",
    "pcds = [convert_to_o3dpcd(pc_fview[i,:,:].cpu().numpy()) for i in range(gt_boxes.shape[0])]\n",
    "\n",
    "origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0,0,0])\n",
    "o3d.visualization.draw_geometries(o3d_boxes + pcds +\n",
    "                                   [origin_frame])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb187ff",
   "metadata": {},
   "source": [
    "# PROTOTYPE - VARIABLE INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5426b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from extensions.chamfer_dist import ChamferDistanceL2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from extensions.chamfer_dist import ChamferDistanceL2\n",
    "from utils.bbox_utils import rot_from_heading, get_dims\n",
    "from extensions.iou3d_nms import iou3d_nms_utils\n",
    "from utils.bbox_utils import get_bbox_from_keypoints\n",
    "\n",
    "def point_maxpool(inputs, npts, keepdims):\n",
    "    # Max pool to get a 256 feature vec for the whole object \n",
    "    # [(1,256,1),(1,256,1),...] of len=batchsize\n",
    "    outputs = [torch.max(f, dim=2, keepdim=keepdims)[0] for f in torch.split(inputs, npts, dim=2)]\n",
    "    return torch.cat(outputs, dim=0)\n",
    "\n",
    "def point_unpool(inputs, npts):\n",
    "    # Assign same 256 features to all points in the original object pcd\n",
    "    # [(1,256,N1),(1,256,N2),...] of len=batchsize\n",
    "    outputs = [torch.tile(f, [1, 1, npts[i]]) for i,f in enumerate(inputs)]\n",
    "\n",
    "    return torch.cat(outputs, dim=2)\n",
    "\n",
    "def fc_layers(layer_dims):\n",
    "    in_channels = layer_dims[0]\n",
    "    fc_layers = []\n",
    "    for out_channel in layer_dims[1:-1]:        \n",
    "        fc_layers += [nn.Linear(in_channels, out_channel),\n",
    "                    nn.BatchNorm1d(out_channel),\n",
    "                    nn.ReLU()]\n",
    "        in_channels = out_channel\n",
    "\n",
    "    fc_layers += [nn.Linear(in_channels, layer_dims[-1])]\n",
    "    return nn.Sequential(*fc_layers)\n",
    "\n",
    "def conv_layers(layer_dims):\n",
    "    in_channels = layer_dims[0]\n",
    "    conv_layers = []\n",
    "    for out_channel in layer_dims[1:]:\n",
    "        conv_layers += [nn.Conv1d(in_channels, out_channel, kernel_size=1),\n",
    "                    nn.BatchNorm1d(out_channel),\n",
    "                    nn.ReLU()]\n",
    "        in_channels = out_channel\n",
    "    return nn.Sequential(*conv_layers)\n",
    "\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, num_keypoints):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.mlp_conv1 = nn.Sequential(\n",
    "            nn.Conv1d(3,128,1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128,256,1)\n",
    "        )\n",
    "        self.mlp_conv2 = nn.Sequential(\n",
    "            nn.Conv1d(512,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,num_keypoints,1)\n",
    "        )    \n",
    "\n",
    "    def forward(self, x, npts_per_id):\n",
    "        # Pytorch is (B,C,N) format\n",
    "\n",
    "        # 1 3 N\n",
    "        mlp_feat = self.mlp_conv1(x) # 1 256 N\n",
    "        symmetric_feat = point_maxpool(mlp_feat, npts_per_id, keepdims=True)  # B 256 1\n",
    "        symmetric_feat = point_unpool(symmetric_feat, npts_per_id)  # 1 256 N\n",
    "        \n",
    "        # Concatenate global (symmetric) and point (mlp) features\n",
    "        features = torch.cat([mlp_feat, symmetric_feat], dim=1) # 1 512 N\n",
    "\n",
    "        # Process the combined features\n",
    "        # 1 global 1024-feature vector per object (i.e. 1024 channels)\n",
    "        combined_feat = self.mlp_conv2(features)  # 1 1024 N\n",
    "        combined_feat = point_maxpool(combined_feat, npts_per_id, keepdims=False)  # B 1024 1\n",
    "        \n",
    "        return combined_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95528f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCKP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.N_keypoints = config.num_keypoints\n",
    "        self.centre_mlp = conv_layers(layer_dims=[3,64,128,256,512])\n",
    "        self.centre_fc = fc_layers(layer_dims=[512,256,128,3])\n",
    "        self.heading_mlp = conv_layers(layer_dims=[3,64,128,256,512])\n",
    "        self.heading_fc = fc_layers(layer_dims=[512,256,128,1])\n",
    "        \n",
    "        self.encoder = FeatureEncoder(self.N_keypoints)    \n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(self.N_keypoints,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,3*self.N_keypoints)\n",
    "        )\n",
    "        # self.loss_regis = ChamferDistanceL2()\n",
    "        self.loss_coarse = ChamferDistanceL2()\n",
    "        self.loss_translation = nn.SmoothL1Loss(reduction='none')\n",
    "        self.loss_dims = nn.SmoothL1Loss(reduction='none')    \n",
    "\n",
    "    \n",
    "    def get_loss(self, ret_dict, in_dict):        \n",
    "\n",
    "        gt_boxes = in_dict['gt_boxes']\n",
    "        \n",
    "        loss_dict = {}        \n",
    "        pred_box = get_bbox_from_keypoints(ret_dict['coarse'], gt_boxes) # B 7        \n",
    "        loss_dict['dims'] = self.loss_dims(gt_boxes[:,3:6].cuda(), pred_box[:,3:6]).mean()\n",
    "\n",
    "        # Translation loss\n",
    "        gt_centres = gt_boxes[:,:3]\n",
    "        loss_dict['translation'] = self.loss_translation(gt_centres.cuda(), ret_dict['reg_centre']).mean()\n",
    "\n",
    "        # Coarse loss - with or without random gt downsampling is not much diff\n",
    "        loss_dict['coarse'] = self.loss_coarse(ret_dict['coarse'], in_dict['complete'])\n",
    "\n",
    "        return loss_dict\n",
    "            \n",
    "    def forward(self, in_dict):\n",
    "        \n",
    "        x = in_dict['input'] # 1 N 3\n",
    "        npts = list(in_dict['num_pts'])\n",
    "        ret = {}\n",
    "\n",
    "        # x input shape is 1 N 3\n",
    "        x_indv = torch.split(x, npts, dim=1) # [x1,x2,x3] each of 1 N 3\n",
    "\n",
    "        # Centre pointcloud on the mean of the points\n",
    "        obj_mean = torch.cat([obj.mean(dim=1).unsqueeze(1) for obj in x_indv], dim=0) # B 1 3\n",
    "        x_meancentered = torch.cat([obj - obj.mean(dim=1).unsqueeze(1) for obj in x_indv],dim=1) # 1 N 3\n",
    "        x_meancentered = x_meancentered.permute(0,2,1) # 1 3 N\n",
    "\n",
    "        # Regress the centre delta                \n",
    "        centre_conv = self.centre_mlp(x_meancentered) # B 3 3, B 1 3       \n",
    "        centre_conv = point_maxpool(centre_conv, npts, keepdims=False)  # B feat_dims\n",
    "        centre_delta = self.centre_fc(centre_conv).unsqueeze(2) # B 3 1\n",
    "        centre = obj_mean + centre_delta.permute(0,2,1) # B 1 3\n",
    "        x_centered_list = [x_indv[i] - centre[i].unsqueeze(1) for i in range(len(x_indv))] # each is 1 N 3\n",
    "        x_centered = torch.cat(x_centered_list, dim=1) # 1 N 3\n",
    "\n",
    "        encoded_feat = self.encoder(x_centered.permute(0,2,1), npts) # B N_keypoints\n",
    "\n",
    "        # Coarse completion\n",
    "        coarse = self.final_mlp(encoded_feat) # B N_keypoints*3\n",
    "        coarse = coarse.reshape([-1, self.N_keypoints, 3]) # B N_keypoints 3\n",
    "\n",
    "        # Get dimensions of coarse keypoints        \n",
    "        ret['reg_centre'] = torch.cat([centre.squeeze(1)], dim=1) # B 3\n",
    "\n",
    "        # Bring back to VC coordinates\n",
    "        coarse_vc = coarse + centre # B N 3\n",
    "        ret['coarse'] = coarse_vc.contiguous()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d09584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 00:18:32,263 - VC_DATASET - INFO - Complete collecting files for train. Total views: 122800\n",
      "  0%|          | 0/7675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.set_device(2) # set to gpu 1080 whenever .cuda() is used\n",
    "\n",
    "cfg_file = '/PoinTr/cfgs/VC_models/VCKP.yaml'\n",
    "config = cfg_from_yaml_file(cfg_file)\n",
    "\n",
    "data_subset = config.dataset.train\n",
    "data_subset.others.num_inputs = 1\n",
    "# data_subset._base_.USE_NVIEWS_PER_MODEL = 1\n",
    "\n",
    "dataset = build_dataset_from_cfg(data_subset._base_, data_subset.others)\n",
    "print(dataset.num_inputs)\n",
    "\n",
    "if data_subset._base_.NAME in ['VC','KITTI']:\n",
    "    if data_subset.others.fixed_input:\n",
    "        collate_fn = dataset.collate_fixed_input\n",
    "    else:            \n",
    "        collate_fn = dataset.collate_variable_input\n",
    "else:\n",
    "    collate_fn=None\n",
    "\n",
    "sampler = None\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16,\n",
    "                                        shuffle = True, \n",
    "                                        drop_last = False,\n",
    "                                        num_workers = 0,\n",
    "                                        worker_init_fn=worker_init_fn,\n",
    "                                        collate_fn=collate_fn)\n",
    "for idx, (taxonomy_ids, model_ids, data) in tqdm(enumerate(dataloader), total=len(dataloader)):        \n",
    "    label = data[2]\n",
    "    in_dict = {}    \n",
    "    if isinstance(data[0], list):\n",
    "        print('using siamese setup')\n",
    "        num_inputs = len(data[0])\n",
    "        for i in range(num_inputs):\n",
    "            in_dict[f'input_{i}'] = data[0][i].cuda()\n",
    "            in_dict[f'complete_{i}'] = data[1][i].cuda()\n",
    "            in_dict[f'gt_boxes_{i}'] = label[i]['gt_boxes'].cuda()\n",
    "            in_dict[f'num_pts_{i}'] = label[i]['num_pts']\n",
    "    else:\n",
    "        in_dict['input'] = data[0].cuda()\n",
    "        in_dict['gt_boxes'] = label['gt_boxes'].cuda() \n",
    "        in_dict['num_pts'] = label['num_pts']\n",
    "        in_dict['complete'] = data[1].cuda()\n",
    "        \n",
    "    in_dict['training'] = True   \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a07b30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_centre': tensor([[  9.8075,  -2.6202,  -1.5221],\n",
       "         [-43.6451,   8.2044,  -1.4281],\n",
       "         [ 59.1736,  27.5230,   0.1612],\n",
       "         [-40.3490,  -0.4222,  -1.4629],\n",
       "         [ 49.4682, -24.5758,  -2.1070],\n",
       "         [ 41.7597, -23.8500,  -0.9562],\n",
       "         [ -5.3340,  41.5594,  -2.4481],\n",
       "         [ 78.3320,  -6.6815,  -2.0265],\n",
       "         [ -2.0735,  11.2635,  -1.3250],\n",
       "         [-73.8981, -19.0240,  -1.8347],\n",
       "         [ 10.0078, -16.1026,  -0.8282],\n",
       "         [ -7.8681,   3.1434,  -1.6257],\n",
       "         [ 32.1085,  15.8986,  -1.9930],\n",
       "         [-11.5924,  -6.6723,  -1.8093],\n",
       "         [  7.6842,  15.6795,  -2.1569],\n",
       "         [-14.0872,  -4.1061,  -1.6119]], device='cuda:2',\n",
       "        grad_fn=<CatBackward>),\n",
       " 'coarse': tensor([[[  9.8036,  -2.5945,  -1.4957],\n",
       "          [  9.8384,  -2.5949,  -1.5703],\n",
       "          [  9.7862,  -2.6076,  -1.4983],\n",
       "          ...,\n",
       "          [  9.7935,  -2.6180,  -1.4893],\n",
       "          [  9.8146,  -2.6370,  -1.5558],\n",
       "          [  9.8178,  -2.6366,  -1.5221]],\n",
       " \n",
       "         [[-43.6496,   8.2366,  -1.4011],\n",
       "          [-43.6211,   8.2229,  -1.4764],\n",
       "          [-43.6684,   8.2132,  -1.4026],\n",
       "          ...,\n",
       "          [-43.6554,   8.2033,  -1.3962],\n",
       "          [-43.6425,   8.1902,  -1.4578],\n",
       "          [-43.6391,   8.1782,  -1.4307]],\n",
       " \n",
       "         [[ 59.1680,  27.5527,   0.1869],\n",
       "          [ 59.2026,  27.5466,   0.1154],\n",
       "          [ 59.1521,  27.5348,   0.1831],\n",
       "          ...,\n",
       "          [ 59.1645,  27.5224,   0.1914],\n",
       "          [ 59.1796,  27.5070,   0.1263],\n",
       "          [ 59.1833,  27.5015,   0.1582]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-11.5980,  -6.6418,  -1.7846],\n",
       "          [-11.5672,  -6.6518,  -1.8533],\n",
       "          [-11.6139,  -6.6620,  -1.7871],\n",
       "          ...,\n",
       "          [-11.6007,  -6.6739,  -1.7773],\n",
       "          [-11.5908,  -6.6871,  -1.8409],\n",
       "          [-11.5847,  -6.6977,  -1.8117]],\n",
       " \n",
       "         [[  7.6810,  15.7095,  -2.1290],\n",
       "          [  7.7141,  15.7031,  -2.2055],\n",
       "          [  7.6641,  15.6920,  -2.1357],\n",
       "          ...,\n",
       "          [  7.6738,  15.6797,  -2.1262],\n",
       "          [  7.6885,  15.6647,  -2.1912],\n",
       "          [  7.6919,  15.6572,  -2.1616]],\n",
       " \n",
       "         [[-14.0930,  -4.0755,  -1.5861],\n",
       "          [-14.0614,  -4.0854,  -1.6571],\n",
       "          [-14.1097,  -4.0956,  -1.5883],\n",
       "          ...,\n",
       "          [-14.0963,  -4.1081,  -1.5800],\n",
       "          [-14.0862,  -4.1206,  -1.6433],\n",
       "          [-14.0811,  -4.1324,  -1.6142]]], device='cuda:2',\n",
       "        grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# npts = list(np.random.randint(50,2048, 1))\n",
    "# sim_data = torch.rand(1,sum(npts),3).cuda()\n",
    "# print(f'input shape = {sim_data.shape}')\n",
    "\n",
    "vckp = VCKP(config.model)\n",
    "vckp.eval()\n",
    "vckp = vckp.cuda()\n",
    "ret = vckp(in_dict)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict = vckp.get_loss(ret, in_dict)\n",
    "print(loss_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666be88",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351b2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import data_transforms\n",
    "from utils.transform import rotate_points_along_z\n",
    "\n",
    "\n",
    "def GlobalRotation(points, gt_boxes, rot_range=[-0.78, 0.78]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gt_boxes: (N, 7 + C), [x, y, z, dx, dy, dz, heading, [vx], [vy]]\n",
    "        points: (M, 3 + C),\n",
    "        rot_range: [min, max]\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    noise_rotation = np.random.uniform(rot_range[0], rot_range[1])\n",
    "    print(no)\n",
    "    points = rotate_points_along_z(points[np.newaxis, :, :], np.array([noise_rotation]))[0]\n",
    "    gt_boxes[0:3] = rotate_points_along_z(gt_boxes[np.newaxis, np.newaxis, 0:3], np.array([noise_rotation]))[0]\n",
    "    gt_boxes[6] += noise_rotation\n",
    "\n",
    "    return points, gt_boxes\n",
    "\n",
    "\n",
    "def GlobalScaling(points, gt_boxes, scale_range=[0.9,1.1]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading]\n",
    "        points: (M, 3 + C),\n",
    "        scale_range: [min, max]\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if scale_range[1] - scale_range[0] < 1e-3:\n",
    "        return gt_boxes, points\n",
    "    noise_scale = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    print(noise_scale)\n",
    "    points[:,:3] *= noise_scale\n",
    "    gt_boxes[3:6] *= noise_scale\n",
    "\n",
    "    return points, gt_boxes\n",
    "\n",
    "def RandomObjectScaling(points, gt_boxes, scale_range=[0.75,1.05]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        gt_boxes: (N, 7), [x, y, z, dx, dy, dz, heading]\n",
    "        points: (M, 3 + C),\n",
    "        scale_range: [min, max]\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if scale_range[1] - scale_range[0] < 1e-3:\n",
    "        return gt_boxes, points\n",
    "    noise_scale = np.random.uniform(scale_range[0], scale_range[1], 3)    \n",
    "    points[:,:3] *= noise_scale\n",
    "    gt_boxes[:6] *= noise_scale\n",
    "\n",
    "    return points, gt_boxes\n",
    "\n",
    "def RandomRingDropout(pts):\n",
    "        \n",
    "    sph_pts = cart2sph(pts)\n",
    "\n",
    "    # Doane or fd bin selection seems to do a good job at finding rings\n",
    "    hist = np.histogram(sph_pts[:,2], bins='doane')\n",
    "    num_rings = len(hist[0])\n",
    "\n",
    "    # Only drop rings when there are rings to drop\n",
    "    if num_rings > 0:                \n",
    "        ringidx = np.digitize(sph_pts[:,2], hist[1])\n",
    "\n",
    "        # Number of rings to drop; min 1 ring remaining\n",
    "        num_drop_rings = np.random.randint(0,num_rings-1,1).item()\n",
    "\n",
    "        # Choose which rings to drop\n",
    "        drop_rings = np.random.randint(0, num_rings, num_drop_rings)\n",
    "        mask = ~np.in1d(ringidx, drop_rings)\n",
    "\n",
    "        if np.count_nonzero(mask) > min_out_pts:\n",
    "            return sph2cart(sph_pts[mask])\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490cd1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 06:55:16,360 - VC_DATASET - INFO - Complete collecting files for train. Total views: 122800\n"
     ]
    }
   ],
   "source": [
    "cfg_file = '/PoinTr/cfgs/VC_models/PCN.yaml'\n",
    "config = cfg_from_yaml_file(cfg_file)\n",
    "\n",
    "data_subset = config.dataset.train\n",
    "dataset = build_dataset_from_cfg(data_subset._base_, data_subset.others)\n",
    "if data_subset._base_.NAME in ['VC','KITTI']:\n",
    "    if data_subset.others.fixed_input:\n",
    "        collate_fn = dataset.collate_fixed_input\n",
    "    else:            \n",
    "        collate_fn = dataset.collate_variable_input\n",
    "else:\n",
    "    collate_fn=None\n",
    "\n",
    "sampler = None\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                        shuffle = False, \n",
    "                                        drop_last = False,\n",
    "                                        num_workers = 0,\n",
    "                                        worker_init_fn=worker_init_fn,\n",
    "                                        collate_fn=collate_fn)\n",
    "for idx, (taxonomy_ids, model_ids, data) in enumerate(dataloader):\n",
    "    \n",
    "    label = data[2]\n",
    "    in_dict = {}\n",
    "    in_dict['input'] = data[0].cuda()\n",
    "    in_dict['gt_boxes'] = label['gt_boxes'].cuda()\n",
    "    in_dict['complete'] = data[1].cuda()\n",
    "    in_dict['training'] = True\n",
    "    in_dict['num_pts'] = label['num_pts'] \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(in_dict['input'].squeeze(0).cpu().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64decaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(data, mask, skip_keys=[]):\n",
    "    size_pos = data.shape[1]\n",
    "    for k in data.keys:\n",
    "        if size_pos == len(data[k]) and k not in skip_keys:\n",
    "            data[k] = data[k][mask]\n",
    "    return data\n",
    "\n",
    "class PeriodicSampling(object):\n",
    "    \"\"\"\n",
    "    sample point at a periodic distance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, period=0.1, prop=0.1, box_multiplier=1, skip_keys=[]):\n",
    "\n",
    "        self.pulse = 2 * np.pi / period\n",
    "        self.thresh = np.cos(self.pulse * prop * period * 0.5)\n",
    "        self.box_multiplier = box_multiplier\n",
    "        self.skip_keys = skip_keys\n",
    "\n",
    "    def __call__(self, data, center):\n",
    "        \n",
    "        d_p = torch.norm(data - center, dim=1)\n",
    "        mask = torch.cos(self.pulse * d_p) > self.thresh\n",
    "        data = data[mask]\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}(pulse={}, thresh={}, box_mullti={}, skip_keys={})\".format(\n",
    "            self.__class__.__name__, self.pulse, self.thresh, self.box_multiplier, self.skip_keys\n",
    "        )\n",
    "    \n",
    "# sampler = PeriodicSampling()\n",
    "# sampler(in_dict['input'].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a9a73836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(145, 3)\n",
      "(145,)\n",
      "(67, 3)\n"
     ]
    }
   ],
   "source": [
    "data = in_dict['input'].cpu().numpy().squeeze(0)\n",
    "center = in_dict['gt_boxes'].cpu().numpy().squeeze(0)[:3][np.newaxis,:]\n",
    "print(center.shape)\n",
    "print(data.shape)\n",
    "# center = np.array([[0,0,0]])\n",
    "\n",
    "d_p = np.linalg.norm(data - center, axis=1)\n",
    "print(d_p.shape)\n",
    "prop = 0.5\n",
    "period = 0.1\n",
    "pulse = 2 * np.pi / period\n",
    "mask = np.cos(pulse * d_p) > np.cos(np.pi * prop)\n",
    "out = data[mask]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9b8da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(in_dict['input'].squeeze(0).cpu().numpy(), [0.9,0.9,0.9]), convert_to_o3dpcd(out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9808d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a74eec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411865883874125"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0.5,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9f91d165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pts = in_dict['input'].squeeze(0).cpu().numpy()\n",
    "sph_pts = cart2sph(pts)\n",
    "_ = plt.hist(sph_pts[:,2], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "26b61c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rings =  29\n",
      "num_rings =  29\n",
      "num chosen rings =  2\n",
      "num output points =  (15, 3)\n"
     ]
    }
   ],
   "source": [
    "hist = np.histogram(sph_pts[:,2], bins=50)\n",
    "ring_indices = np.digitize(sph_pts[:,2], hist[1][np.argwhere(hist[0] > 0).squeeze(1)])\n",
    "print('num rings = ', len(np.argwhere(hist[0] > 0).squeeze(1)))\n",
    "num_rings = max(ring_indices)\n",
    "print('num_rings = ', num_rings)\n",
    "choose_rings = np.unique(ring_indices)[np.random.randint(0,3)::np.random.randint(1,num_rings)]\n",
    "mask = np.in1d(ring_indices, choose_rings)\n",
    "print('num chosen rings = ', len(choose_rings))\n",
    "out = sph2cart(sph_pts[mask])\n",
    "print('num output points = ', out.shape)\n",
    "\n",
    "if np.count_nonzero(mask) > 5:\n",
    "    o3d.visualization.draw_geometries([convert_to_o3dpcd(out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7de9af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[1 8]\n",
      "(137,)\n",
      "(137,)\n"
     ]
    }
   ],
   "source": [
    "pts = in_dict['input'].squeeze(0).cpu().numpy()\n",
    "sph_pts = cart2sph(pts)\n",
    "\n",
    "# Doane or fd bin selection seems to do a good job at finding rings\n",
    "hist = np.histogram(sph_pts[:,2], bins='doane')\n",
    "print(len(hist[0]))\n",
    "ringidx = np.digitize(sph_pts[:,2], hist[1])\n",
    "num_rings = max(ringidx)\n",
    "# # Number of rings to drop; min 1 ring remaining\n",
    "# num_drop_rings = np.random.randint(0,num_rings-1,1).item()\n",
    "\n",
    "# # Choose which rings to drop\n",
    "# drop_rings = np.random.randint(0, num_rings, num_drop_rings)\n",
    "\n",
    "choose_rings = np.unique(ringidx)[np.random.randint(0,3)::np.random.randint(1,num_rings)]\n",
    "print(choose_rings)\n",
    "mask = np.in1d(ringidx, choose_rings)\n",
    "print(ringidx.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "out = sph2cart(sph_pts[mask])\n",
    "# if np.count_nonzero(mask) > min_out_pts:\n",
    "#     return \n",
    "\n",
    "# return pts\n",
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(out)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c84055",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtbox = np.random.rand(7)\n",
    "partial = np.random.rand(1024,3)\n",
    "complete = np.random.rand(16384,3)\n",
    "\n",
    "partial_cn = rotate_points_along_z((partial - gtbox[:3])[np.newaxis, :, :], np.array([-gtbox[-1]]))[0]\n",
    "complete_cn = rotate_points_along_z((complete - gtbox[:3])[np.newaxis, :, :], np.array([-gtbox[-1]]))[0]\n",
    "\n",
    "noise_scale = np.random.uniform(0.7, 1, 3)    \n",
    "partial_cn[:,:3] *= noise_scale\n",
    "complete_cn[:,:3] *= noise_scale\n",
    "gtbox[3:6] *= noise_scale\n",
    "\n",
    "# Transform back to view-centric frame\n",
    "p = rotate_points_along_z(partial_cn[np.newaxis, :, :], np.array([gtbox[-1]]))[0] + gtbox[:3]\n",
    "c = rotate_points_along_z(complete_cn[np.newaxis, :, :], np.array([gtbox[-1]]))[0] + gtbox[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e5cd45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.2513878 , 1.2162952 , 1.1819357 ],\n",
       "        [0.6090073 , 0.9188445 , 1.0006177 ],\n",
       "        [0.7022839 , 0.83695686, 0.9074545 ],\n",
       "        ...,\n",
       "        [0.9824947 , 1.7000049 , 1.1866556 ],\n",
       "        [1.0908313 , 1.2706375 , 1.2933723 ],\n",
       "        [0.55828077, 1.5947784 , 1.0871577 ]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vc_to_cn(points, gt_label):\n",
    "    \"\"\"\n",
    "    Transform the given points from view-centric to canonical frame\n",
    "\n",
    "    For view-centric to canonical, use -angle\n",
    "    For canonical to view-centric, use +angle\n",
    "\n",
    "    Args:\n",
    "        points: (B, N, 3 + C)\n",
    "        gt_label: (B, 7), angle along z-axis, angle increases x ==> y\n",
    "    \"\"\"\n",
    "    points, is_numpy = check_numpy_to_torch(points)\n",
    "    gt_label, _ = check_numpy_to_torch(gt_label)\n",
    "\n",
    "    centre = gt_label[:,:3]\n",
    "    centre_points = points - centre\n",
    "    points_cn = rotate_points_along_z(centre_points, -gt_label[:,-1])\n",
    "\n",
    "    return points_cn.numpy() if is_numpy else points_cn\n",
    "\n",
    "def vc_to_cn(points, gt_label):\n",
    "    \"\"\"\n",
    "    Transform the given points from canonical frame to view-centric\n",
    "\n",
    "    For view-centric to canonical, use -angle\n",
    "    For canonical to view-centric, use +angle\n",
    "\n",
    "    Args:\n",
    "        points: (B, N, 3 + C)\n",
    "        gt_label: (B, 7), angle along z-axis, angle increases x ==> y\n",
    "    \"\"\"\n",
    "    points, is_numpy = check_numpy_to_torch(points)\n",
    "    gt_label, _ = check_numpy_to_torch(gt_label)\n",
    "\n",
    "    centre = gt_label[:,:3]    \n",
    "    points_rot = rotate_points_along_z(points, gt_label[:,-1])\n",
    "    points_vc = points + centre\n",
    "\n",
    "    return points_vc.numpy() if is_numpy else points_vc\n",
    "\n",
    "gt_label = np.random.rand(7)[np.newaxis, :]\n",
    "partial = np.random.rand(1024,3)[np.newaxis, :]\n",
    "vc_to_cn(partial, gt_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae1d3223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2461/4120801301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgt_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mgt_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'not'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "gt_label = np.random.rand(7)\n",
    "assert gt_label.shape == (1,7), print('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a67ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,data = dataset.__getitem__(6)\n",
    "\n",
    "pts = data[0].cpu().numpy()\n",
    "gpts = data[1].cpu().numpy()\n",
    "gtbox = data[2]['gtbox']\n",
    "boxpts = opd_to_boxpts(gtbox)\n",
    "o3dgtbox = boxpts_to_o3dbox(boxpts, [1,0.9,0])\n",
    "\n",
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(pts), convert_to_o3dpcd(gpts, [0.9,0.9,0.9]), o3dgtbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6d83599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.1170,  1.7164,  0.1227],\n",
      "        [ 2.1340,  1.7229,  0.0981],\n",
      "        [ 2.1294,  1.6747,  0.0923],\n",
      "        ...,\n",
      "        [ 1.1036,  3.0881, -0.1043],\n",
      "        [ 1.1691,  3.0742, -0.0870],\n",
      "        [ 1.1512,  3.0886, -0.1043]])\n",
      "tensor([[ 1.9035,  1.5433,  0.1103],\n",
      "        [ 1.9187,  1.5491,  0.0882],\n",
      "        [ 1.9146,  1.5058,  0.0830],\n",
      "        ...,\n",
      "        [ 0.9922,  2.7766, -0.0938],\n",
      "        [ 1.0511,  2.7641, -0.0782],\n",
      "        [ 1.0351,  2.7771, -0.0938]])\n"
     ]
    }
   ],
   "source": [
    "norm_pts = rotate_points_along_z((pts - gtbox[:,:3]).unsqueeze(0), -gtbox[:,-1]).squeeze(0)\n",
    "print(norm_pts)\n",
    "norm_pts[:,:3] *= noise_scale\n",
    "print(norm_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b6f4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,data = dataset.__getitem__(37)\n",
    "\n",
    "pts = data[1].cpu()\n",
    "\n",
    "# gpts = data[1].cpu()\n",
    "gtbox = torch.from_numpy(data[2]['gtbox']).float().unsqueeze(0)\n",
    "norm_pts = rotate_points_along_z((pts - gtbox[:,:3]).unsqueeze(0), -gtbox[:,-1]).squeeze(0)\n",
    "\n",
    "noise_scale = np.random.uniform(0.75, 1.1, 3)   \n",
    "aug_pts = norm_pts.clone()\n",
    "aug_pts[:,:3] *= noise_scale\n",
    "\n",
    "origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0,0,0])\n",
    "o3d.visualization.draw_geometries([convert_to_o3dpcd(norm_pts, [1,1,0]), convert_to_o3dpcd(aug_pts, [1,0,0]), origin_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bed3faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-25.6106,  12.1614,  -1.6205])\n",
      "tensor([ 0.1252, -0.0099,  0.0460])\n",
      "[0.8973763  0.86145016 1.07060234]\n",
      "tensor([ 0.1124, -0.0086,  0.0493])\n",
      "tensor([[-25.7084,  12.2402,  -1.6665,   3.4060,   1.8033,   1.4007,  -0.5987]])\n",
      "tensor([[-25.7084,  12.2402,  -1.6665,   3.0565,   1.5535,   1.4996,  -0.5987]])\n",
      "tensor([-25.6205,  12.1698,  -1.6172])\n"
     ]
    }
   ],
   "source": [
    "_,_,data = dataset.__getitem__(0)\n",
    "\n",
    "pts = data[1].cpu()\n",
    "print(pts.mean(dim=0))\n",
    "\n",
    "# gpts = data[1].cpu()\n",
    "gtbox = torch.from_numpy(data[2]['gtbox']).float().unsqueeze(0)\n",
    "norm_pts = rotate_points_along_z((pts - gtbox[:,:3]).unsqueeze(0), -gtbox[:,-1]).squeeze(0)\n",
    "\n",
    "print(norm_pts.mean(dim=0))\n",
    "noise_scale = np.random.uniform(0.75, 1.1, 3)   \n",
    "print(noise_scale)\n",
    "norm_pts[:,:3] *= noise_scale\n",
    "print(norm_pts.mean(dim=0))\n",
    "\n",
    "vc_pts = rotate_points_along_z(norm_pts.unsqueeze(0), gtbox[:,-1]).squeeze(0) + gtbox[:,:3]\n",
    "augbox = gtbox.clone()\n",
    "augbox[:,3:6] *= noise_scale\n",
    "print(gtbox)\n",
    "print(augbox)\n",
    "print(vc_pts.mean(dim=0))\n",
    "\n",
    "# Viz\n",
    "boxpts = opd_to_boxpts(gtbox.squeeze(0).cpu().numpy())\n",
    "o3dgtbox = boxpts_to_o3dbox(boxpts, [1,0.9,0])\n",
    "\n",
    "boxpts = opd_to_boxpts(augbox.squeeze(0).cpu().numpy())\n",
    "o3daugbox = boxpts_to_o3dbox(boxpts, [1,0,0])\n",
    "o3d.visualization.draw_geometries([o3dgtbox, o3daugbox, convert_to_o3dpcd(vc_pts, [1,0,0]), convert_to_o3dpcd(pts.numpy(), [1,0.9,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4f4fc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98511624 0.84017119 0.81044928]\n"
     ]
    }
   ],
   "source": [
    "# _,_,data = dataset.__getitem__(12)\n",
    "\n",
    "# pts = data[0].cpu()\n",
    "# gtbox = torch.from_numpy(data[2]['gtbox']).float().unsqueeze(0)\n",
    "# gtbox_norm = gtbox.clone()\n",
    "# gtbox_norm[:,:3] = 0\n",
    "# gtbox_norm[:,-1] = 0\n",
    "# boxpts = opd_to_boxpts(gtbox_norm.squeeze(0).cpu().numpy())\n",
    "# o3dgtbox = boxpts_to_o3dbox(boxpts, [0,1,1])\n",
    "\n",
    "# norm_pts = rotate_points_along_z((pts - gtbox[:,:3]).unsqueeze(0), -gtbox[:,-1])\n",
    "# in_pcd = convert_to_o3dpcd(norm_pts.squeeze(0).cpu().numpy())\n",
    "# in_pcd.paint_uniform_color([0.9,0.9,0.9])\n",
    "\n",
    "# s = np.random.uniform(0.75,1.05, 3)\n",
    "# print(s)\n",
    "# augpts = norm_pts.clone().squeeze(0).cpu().numpy()\n",
    "# augpts[:,0] *= s[0]\n",
    "# augpts[:,1] *= s[1]\n",
    "# augpts[:,2] *= s[2]\n",
    "# aug_pcd = convert_to_o3dpcd(augpts)\n",
    "# aug_box = boxpts.copy()\n",
    "# aug_box[:,0] *= s[0]\n",
    "# aug_box[:,1] *= s[1]\n",
    "# aug_box[:,2] *= s[2]\n",
    "# o3daug_box = boxpts_to_o3dbox(aug_box, [1,0,0])\n",
    "\n",
    "# origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0,0,0])\n",
    "# o3d.visualization.draw_geometries([in_pcd, aug_pcd, o3daug_box, o3dgtbox, origin_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15b5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5, origin=[0,0,0])\n",
    "o3d.visualization.draw_geometries([in_pcd, aug_pcd, o3dgtbox, origin_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf2e0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0076405819205165\n"
     ]
    }
   ],
   "source": [
    "_,_,data = dataset.__getitem__(10)\n",
    "\n",
    "pts = data[0].cpu().numpy()\n",
    "gtbox = np.array(data[2]['gtbox'])\n",
    "boxpts = opd_to_boxpts(data[2]['gtbox'])\n",
    "o3dbox = boxpts_to_o3dbox(boxpts, [0,1,1])\n",
    "in_pcd = convert_to_o3dpcd(pts)\n",
    "in_pcd.paint_uniform_color([0.9,0.9,0.9])\n",
    "\n",
    "augpts, augbox = GlobalScaling(pts, gtbox)\n",
    "\n",
    "aug_pcd = convert_to_o3dpcd(augpts)\n",
    "boxpts = opd_to_boxpts(augbox)\n",
    "aug_o3dbox = boxpts_to_o3dbox(boxpts)\n",
    "\n",
    "o3d.visualization.draw_geometries([in_pcd, o3dbox, aug_pcd, aug_o3dbox, origin_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0e829fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.12440401, 1.91832252, 1.18397709])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_o3dbox.extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a817b657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.40466591, 2.04867655, 1.26443081])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3dbox.extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca705f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,data = dataset.__getitem__(0)\n",
    "\n",
    "in_pcd = convert_to_o3dpcd(pts)\n",
    "in_pcd.paint_uniform_color([0.9,0.9,0.9])\n",
    "boxpts = opd_to_boxpts(data[2]['gtbox'])\n",
    "o3dbox = boxpts_to_o3dbox(boxpts)\n",
    "\n",
    "o3d.visualization.draw_geometries([in_pcd, o3dbox])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6564ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_boxes = data[2]['gtbox']\n",
    "points = data[0]\n",
    "fgtbox, fpoints = RandomFlipAlongY(gt_boxes, points)\n",
    "\n",
    "in_pcd2 = convert_to_o3dpcd(fpoints)\n",
    "boxpts = opd_to_boxpts(fgtbox)\n",
    "o3dbox2 = boxpts_to_o3dbox(boxpts)\n",
    "\n",
    "o3d.visualization.draw_geometries([in_pcd, o3dbox, in_pcd2, o3dbox2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf12fdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss_dict.get('he', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80b9b325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9728, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict['dims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "652fd88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8e26cd1550>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred1 = np.array([0.0 for i in range(250)])\n",
    "pred2 = np.linspace(0,2*np.pi, 250)\n",
    "cos1 = np.cos(pred1)\n",
    "cos2 = np.cos(pred2)\n",
    "plt.scatter(pred2, abs(cos1-cos2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "aef52140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007998443909987829"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading_err = -1.7499 - 1.5708\n",
    "cos_sim = (1.0 + np.cos(heading_err))/2.0\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5cbe3da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f99bed5bd10>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heading_err = pred1 - pred2\n",
    "cos_sim = (1.0 + np.cos(heading_err))/2.0\n",
    "plt.scatter(pred2, cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "71899663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f99bedce850>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torchloss = nn.MSELoss(reduction='none')\n",
    "l1loss = torchloss(torch.from_numpy(cos1), torch.from_numpy(cos2))\n",
    "plt.scatter(pred2, l1loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcc0be",
   "metadata": {},
   "source": [
    "# PCNSSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61eeeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from extensions.chamfer_dist import ChamferDistanceL2\n",
    "    \n",
    "def mlp_conv(in_channels, layer_dims, bn=None, bn_params=None):\n",
    "    layers = []\n",
    "    for i, out_channel in enumerate(layer_dims[:-1]):\n",
    "        layers += [nn.Conv1d(in_channels, out_channel, kernel_size=1)]\n",
    "        in_channels = out_channel\n",
    "\n",
    "    layers += [nn.Conv1d(in_channels, layer_dims[-1], kernel_size=1)]\n",
    "    mlp_block = nn.Sequential(*layers)\n",
    "    \n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class PCNencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PCNencoder, self).__init__()\n",
    "        self.mlp_conv1 = nn.Sequential(\n",
    "            nn.Conv1d(3,128,1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(128,256,1)\n",
    "        )\n",
    "        self.mlp_conv2 = nn.Sequential(\n",
    "            nn.Conv1d(512,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,1024,1)\n",
    "        )\n",
    "        \n",
    "    def point_maxpool(self, inputs, npts, keepdims):\n",
    "        # Max pool to get a 256 feature vec for the whole object \n",
    "        # [(1,256,1),(1,256,1),...] of len=batchsize\n",
    "        outputs = [torch.max(f, dim=2, keepdim=keepdims)[0] for f in torch.split(inputs, npts, dim=2)]\n",
    "        return torch.cat(outputs, dim=0)\n",
    "    \n",
    "    def point_unpool(self, inputs, npts):\n",
    "        # Assign same 256 features to all points in the original object pcd\n",
    "        # [(1,256,N1),(1,256,N2),...] of len=batchsize\n",
    "        outputs = [torch.tile(f, [1, 1, npts[i]]) for i,f in enumerate(inputs)]\n",
    "\n",
    "        return torch.cat(outputs, dim=2)\n",
    "\n",
    "    def forward(self, x, npts_per_id):\n",
    "        # Pytorch is (B,C,N) format\n",
    "        # input is (1,3,sum(Ni->b)), Ni refers to the specific num pts in an object\n",
    "        print(x.shape)\n",
    "        mlp_feat = self.mlp_conv1(x) # 1 256 N\n",
    "        print(mlp_feat.shape)\n",
    "        symmetric_feat = self.point_maxpool(mlp_feat, npts_per_id, keepdims=True)  # [(1,256,1),(1,256,1),...] of len=B\n",
    "        symmetric_feat = self.point_unpool(symmetric_feat, npts_per_id)  # (1,256,sum(Ni->b)) \n",
    "        \n",
    "        # Concatenate global (symmetric) and point (mlp) features\n",
    "        features = torch.cat([mlp_feat, symmetric_feat], dim=1)\n",
    "\n",
    "        # Process the combined features\n",
    "        combined_feat = self.mlp_conv2(features)  # (1,1024,sum(Ni->b))\n",
    "        combined_feat = self.point_maxpool(combined_feat, npts_per_id, keepdims=False)  # (B,1024,1)\n",
    "        # 1 global 1024-feature vector per object (i.e. 1024 channels)\n",
    "        return combined_feat\n",
    "\n",
    "class PCNdecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PCNdecoder, self).__init__()\n",
    "        self.grid_scale = 0.05\n",
    "        self.grid_size = 4\n",
    "        self.num_coarse = 1024\n",
    "        self.num_fine = self.grid_size ** 2 * self.num_coarse\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.num_coarse,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,3*self.num_coarse)\n",
    "        )\n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv1d(1024+3+2,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,512,1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(512,3,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Coarse completion\n",
    "        coarse = self.mlp(x) # (B,num_coarse*3)\n",
    "        coarse = coarse.reshape([-1, 3, self.num_coarse]) # We set num_coarse*3 above so that we can bring it back to 3 channels\n",
    "        \n",
    "        # --- Fine completion ---\n",
    "        # 2D grid u x u for each coarse point\n",
    "        u = torch.linspace(start=-self.grid_scale, end=self.grid_scale, steps=self.grid_size).cuda() # create 4x4 grid\n",
    "        grid = torch.meshgrid(u,u) # (4,4) and (4,4)\n",
    "        grid = torch.unsqueeze(torch.reshape(torch.stack(grid, dim=2), (2,-1)),0) # (1,16,2)\n",
    "        folding_seed = torch.tile(grid, [x.shape[0], 1, self.num_coarse]) # grid*num_coarse\n",
    "        \n",
    "        point_feat = torch.tile(torch.unsqueeze(coarse, 2), [1, 1, self.grid_size **2, 1])\n",
    "        point_feat = point_feat.reshape([-1, 3, self.num_fine])\n",
    "        \n",
    "        global_feat = torch.tile(x.unsqueeze(2), [1, 1, self.num_fine])\n",
    "        feat = torch.cat([folding_seed, point_feat, global_feat], dim=1)\n",
    "        center = torch.tile(torch.unsqueeze(coarse, 2), [1, 1, self.grid_size ** 2, 1])\n",
    "        center = center.reshape([-1, 3, self.num_fine])\n",
    "        \n",
    "        fine = self.final_conv(feat) + center\n",
    "\n",
    "        return coarse, fine\n",
    "\n",
    "class PCNSSC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = PCNencoder()\n",
    "        self.decoder = PCNdecoder()\n",
    "        self.build_loss_func()\n",
    "\n",
    "    def build_loss_func(self):\n",
    "        self.loss_func = ChamferDistanceL2()\n",
    "\n",
    "    def get_loss(self, ret, gt):\n",
    "        gt_ds = gt[:, :ret[0].shape[1], :] # random downsample\n",
    "        loss_coarse = self.loss_func(ret[0], gt_ds)\n",
    "        loss_fine = self.loss_func(ret[1], gt)\n",
    "        return loss_coarse, loss_fine\n",
    "            \n",
    "    def forward(self, x, npts):\n",
    "        x = x.permute(0,2,1)\n",
    "        encoded_feats = self.encoder(x, npts)\n",
    "        coarse, fine = self.decoder(encoded_feats)\n",
    "        return (coarse.transpose(1,2).contiguous(), fine.transpose(1,2).contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9847e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 04:26:12,473 - VC_DATASET - INFO - Complete collecting files for train. Total views: 3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4096])\n",
      "torch.Size([1, 256, 4096])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from utils.misc import *\n",
    "dataset = build_dataset_from_cfg(config.dataset.train._base_, config.dataset.train.others)\n",
    "shuffle = config.dataset.train.others.subset == 'train'\n",
    "sampler = None\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2,\n",
    "                                        shuffle = shuffle, \n",
    "                                        drop_last = config.dataset.train.others.subset == 'train',\n",
    "                                        num_workers = 4,\n",
    "                                        worker_init_fn=worker_init_fn,\n",
    "                                        collate_fn=collate_variable_input)\n",
    "\n",
    "model = PCNSSC()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device, dtype=torch.float)\n",
    "\n",
    "_, model_ids, data = next(iter(dataloader))\n",
    "partial = data[0].to(device, dtype=torch.float)\n",
    "complete = data[1].to(device, dtype=torch.float)\n",
    "out = model(partial, data[2]['num_pts'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
